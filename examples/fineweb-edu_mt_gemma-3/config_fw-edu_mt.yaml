# === SLURM Configuration ===
# Consult your cluster's documentation if you are unsure about the values below.
job_name: "fw-edu-mt"
partition: "boost_usr_prod"
account: "AIFAC_L01_028"
qos: "boost_qos_dbg"
cpus_per_node: 32 # cpu is not really needed for our workload.
memory_per_node: "0" # "0" for all available memory per node
gres_per_node: "gpu:4" # Generic resources per node. Examples: "gpu:4", "a100:1", "gpu:h100:8"
time_limit: "29:00" # Max walltime on cluster. Jobs are automatically resubmitted if they exceed time limit.
# Additional SBATCH arguments (optional)
additional_sbatch_args:
  exclude: "lrdn[0032,0701]"  # Exclude specific nodes


# === Scaling Configuration ===
# The total number of nodes will be num_inference_servers * num_nodes_per_inference_server.
num_inference_servers: 2
num_nodes_per_inference_server: 1 # For now we support only 1 node per inference server.

# Each inference_server processes one shard of data. For very large datasets you can opt for more shards than inference servers for checkpointing. However, only num_inference_servers shards will be processed in parallel.
# This option can also be useful if you want to continue an incomplete run with a different number of inference servers.
num_data_shards: # leave empty to use num_inference_servers


# === Environment Configuration ===
pixi_manifest: "pixi.toml"  # Path to the pixi manifest file. Edit only if you want to use one of the pre-installed environments (see README.md).
pixi_env: "cuda-vllm"  # Name of the pixi environment to use (see README.md).
env_vars:
  HF_HUB_OFFLINE: "1" # you should download data and model beforehand
  VLLM_DISABLE_COMPILE_CACHE: "1" # multiple vLLM instances on shared filesystem can cause problems


# === Inference Server Configuration ===
inference_server_command: > 
  python -m vllm.entrypoints.openai.api_server
  --host=localhost
  --port=64776
  --model="google/gemma-3-27b-it"
  --tensor-parallel-size=4
  --data-parallel-size=1
  --max-model-len=32768
  --trust-remote-code
  --gpu-memory-utilization=0.9
  --disable-log-requests



# === Health Check Configuration ===
# max_wait_minutes is the maximum time to wait for the API server to become healthy.
# make sure it is reasonable for loading the model. larger models will take longer.
health_check_max_wait_minutes: 20  # Maximum time to wait for API server to become healthy.
health_check_interval_seconds: 30  # Interval between health check attempts


# === API Configuration ===
# make sure port is the same as in inference_server_command.
api_base_url: "http://localhost:64776/v1"
api_type: "chat-completion"  # "chat-completion" or "completion"
# make sure model is the same as in inference_server_command.
model: "google/gemma-3-27b-it"


# === Dataset Configuration ===
dataset_type: "hf-disk"
dataset_path: "/leonardo_work/AIFAC_L01_028/midahl00/inference-hive/fineweb-edu-mt-chat-completion" # hf identifier or local path
# input_column_name must contain strings for completion or messages for chat-completion.
input_column_name: "conversation"
# id_column_name must contain unique string identifiers for each row. The IDs will be used to match responses with input rows.
id_column_name: "id"

# Dataset Loading Arguments
dataset_kwargs:
  # name: "deu_Latn"
  # split: "train"
  # cache_dir: "/path/to/cache"


# === Output Configuration ===
output_path: "/leonardo_work/AIFAC_L01_028/midahl00/inference-hive/example_outputs/fw-edu-100k-responses-gemma-3-27b" # responses will be saved here


# === Completion Arguments ===
# for completion and chat-completion
completions_kwargs:
  temperature: 0.3
  # max_tokens: 100
  # top_p: 1.0
  # frequency_penalty: 0.0
  # presence_penalty: 0.0
  extra_body: {continue_final_message: true, "add_generation_prompt": false} # we need to set these two for assistant prefill to work.


# === Connection Settings ===
max_connections: 100
max_retries: 3
